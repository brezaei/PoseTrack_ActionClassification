{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the serialized detection results for the target using provided ground truth tracklet labels\n",
    "It is a pre-processing step for generating the pose representation maps for the action recognition. the output of this code will be saved as .pkl format in the output folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import cPickle as pickle\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import pylab\n",
    "import math\n",
    "import time\n",
    "import pylab\n",
    "import shutil\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of requires functions and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _id_or_index(ix, val):\n",
    "    if len(val) == 0:\n",
    "        return val\n",
    "    else:\n",
    "        return val[ix]\n",
    "def robust_pickle_dump(data_dict, file_name):\n",
    "    file_name = os.path.abspath(file_name)\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(data_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "def find_element_in_list(element, list_element):\n",
    "    \"\"\"\n",
    "    finding index of first appearance of an element in a given list\n",
    "    if the specified element does not exist returns None\n",
    "    inputs:\n",
    "        element: query element to be found\n",
    "        list_element: input list for searching the query\n",
    "    outputs:\n",
    "        index_element: index of the query elemnt in the list\n",
    "    \"\"\"\n",
    "    try:\n",
    "        index_element = list_element.index(element)\n",
    "        return index_element\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "# finding the bounding boxes of the specified target\n",
    "def _get_target_tracklet(track_id, all_boxes, all_tracks, st_fr=None):\n",
    "    \"\"\"\n",
    "    finding the target tracklet which is the list of the bounding boxes in consecutive\n",
    "    frames which locate same person by given the track_id of the target\n",
    "    input:\n",
    "        st_fr: start frame of the target appearance\n",
    "        track_id: track ID of the target\n",
    "        all_boxes: the list of the all detected bounding boxes with the human detection person\n",
    "        all_tracks: list of all the track IDs for the detected bounding boxes\n",
    "    output:\n",
    "        end_fr: frame number of the end of a tracklet\n",
    "        st_fr: frame number of the start of a tracklet\n",
    "        target_box: list of the bounding boxes surrounding the target\n",
    "\n",
    "    \"\"\"\n",
    "    if st_fr is None:\n",
    "        search = True\n",
    "        st_fr = 0\n",
    "        while search:\n",
    "            indx = find_element_in_list(track_id, all_tracks[st_fr])\n",
    "            if indx is None:\n",
    "                st_fr += 1\n",
    "            else:\n",
    "                search = False\n",
    "    fr_last = len(all_tracks) - 1\n",
    "    count = st_fr\n",
    "    exist = True\n",
    "    target_box = []\n",
    "    while exist:\n",
    "        indx = find_element_in_list(track_id, all_tracks[count])\n",
    "        if indx is not None:\n",
    "            target_box.append(all_boxes[count][indx])\n",
    "            if count < fr_last:\n",
    "                count += 1\n",
    "            elif count == fr_last:\n",
    "                end_fr = count\n",
    "                exist = False\n",
    "        else:\n",
    "            end_fr = count\n",
    "            exist = False\n",
    "    return end_fr, st_fr, target_box\n",
    "def boxes_area(boxes):\n",
    "    \"\"\"\n",
    "    calculating the area of the input bounding boxes\n",
    "    :param boxes: the list of bounding boxes in the form [x0, y0, x1, y1, conf(optional)]\n",
    "    :return: the area calculated for each bounding box in the list\n",
    "    \"\"\"\n",
    "    w = (boxes[:, 2] - boxes[:, 0] + 1)\n",
    "    h = (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "    areas = w * h\n",
    "    assert np.all(areas >= 0), 'Negative areas founds'\n",
    "    return areas\n",
    "\n",
    "\n",
    "def bbox_overlaps(boxes, query_boxes):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    boxes: (N, 4) ndarray of float\n",
    "    query_boxes: (K, 4) ndarray of float\n",
    "    Returns\n",
    "    -------\n",
    "    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    N = boxes.shape[0]\n",
    "    # print('N={}'.format(N))\n",
    "    K = query_boxes.shape[0]\n",
    "    # print('K={}'.format(K))\n",
    "    overlaps = np.zeros((N, K), dtype=DTYPE)\n",
    "    for k in range(K):\n",
    "        box_area = (\n",
    "                (query_boxes[k, 2] - query_boxes[k, 0] + 1) *\n",
    "                (query_boxes[k, 3] - query_boxes[k, 1] + 1)\n",
    "        )\n",
    "        for n in range(N):\n",
    "            iw = (\n",
    "                    min(boxes[n, 2], query_boxes[k, 2]) -\n",
    "                    max(boxes[n, 0], query_boxes[k, 0]) + 1\n",
    "            )\n",
    "            if iw > 0:\n",
    "                ih = (\n",
    "                        min(boxes[n, 3], query_boxes[k, 3]) -\n",
    "                        max(boxes[n, 1], query_boxes[k, 1]) + 1\n",
    "                )\n",
    "                if ih > 0:\n",
    "                    ua = float(\n",
    "                        (boxes[n, 2] - boxes[n, 0] + 1) *\n",
    "                        (boxes[n, 3] - boxes[n, 1] + 1) +\n",
    "                        box_area - iw * ih\n",
    "                    )\n",
    "                    overlaps[n, k] = iw * ih / ua\n",
    "    return overlaps\n",
    "DTYPE = np.float32\n",
    "height = 1080  # frame height in pixel\n",
    "width = 1920  # frame width in pixel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary containing the name of the all pose estimation results for video dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_det_track_results = {'03121601001':['Kinect_1484230300411_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1484236642988_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601002':['Kinect_1484837025135_Video_Flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1484852850634_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601003':['Kinect_1485442263970_Fixed_Video_Flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1485453235022_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601004':['Kinect_1486044609777_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1486049379590_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601005':['Kinect_1486055718051_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1486062723338_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601006':['Kinect_1487170521651_Fixed_Video_Flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1487252582676_Fixed_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601007':['Kinect_1487773684672_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1488375395995_Fixed_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601008':['Kinect_1487859349689_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1487864851230_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601009':['Kinect_1488985159337_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1488991782221_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601010':['Kinect_1489081164119_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1489582979282_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601011':['Kinect_1489410472438_Video_flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1489416963560_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601012':['Kinect_1490279720993_Video_flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1490299676346_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601013':['Kinect_1489586406980_Video_Flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1489596563256_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601014':['Kinect_1490793487534_Video_flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1491481607487_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601015':['Kinect_1490879543456_Video_Flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1490885684191_video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601016':['Kinect_1491399264372_Video_flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1491408408518_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601017':['Kinect_1491571272034_Video_flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1491576389630_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601018':['Kinect_1492697256228_Video_Flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1493126049741_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601019':['Kinect_1493905943020_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1494422856653_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601020':['Kinect_1492786250052_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1492792155821_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601021':['Kinect_1493214639781_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1493222216696_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601022':['Kinect_1495028284074_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1495037457240_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601023':['Kinect_1495638523488_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1495649327569_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601024':['Kinect_1495638523488_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1495646631951_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601025':['Kinect_1495724260698_Video_flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1496323552600_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601027':['Kinect_1503497547252_Video_Flipped_detections_withTracks.pkl',\n",
    "                              'Kinect_1504101708237_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601028':['Kinect_1503581189725_Video_Flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1504181872815_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601029':['Kinect_1504627411616_Video_Flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1504635058283_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601030':['Kinect_1507211606614_Video_Flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1507220967931_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601031':['Kinect_1508335131827_Video_Flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1508348636573_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601032':['Kinect_1510160593496_Video_Flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1510168880679_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601033':['Kinect_1512057807777_Video_Flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1512068815485_Video_flipped_detections_withTracks.pkl'],\n",
    "               '03121601034':['Kinect_1516897009358_Video_Flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1516904509468_Video_Flipped_detections_withTracks.pkl'],\n",
    "               '03121601035':['Kinect_1522328709409_Video_flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1522335428199_Video_flipped_detections_withTracks.pkl'],         \n",
    "               '03121601036':['Kinect_1524143477281_Video_flipped_detections_withTracks.pkl',\n",
    "                             'Kinect_1524156291481_Video_flipped_detections_withTracks.pkl']\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading detection and tracking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing output results for patient ID:03121601034 saved in Kinect_1516897009358_Video_Flipped_detections_withTracks.pkl \n",
      "There are 924 individual tracks  in 109006 total number of frames\n"
     ]
    }
   ],
   "source": [
    "base_path = '../../../PoseEstimation/DetectAndTrack/DetectAndTrack/outputs/TuftsVideo/HUBB121601_Tufts_Kinect_Videos_Flipped'\n",
    "subj_id = '03121601034'\n",
    "visit_no = 1\n",
    "file_name = all_det_track_results[subj_id][visit_no - 1]\n",
    "print('processing output results for patient ID:{} saved in {} ').format(subj_id, file_name)\n",
    "det_track_path = osp.join(base_path, subj_id, 'Visit_%02d' % visit_no, file_name)\n",
    "with open(det_track_path, 'rb') as res:\n",
    "    dets = pickle.load(res)\n",
    "all_boxes = dets['all_boxes'][1] # bbox:[x0, y0, x1, y1, score]: w=x1-x0; h=y1-y0\n",
    "cfg = dets['cfg']\n",
    "if 'all_keyps' in dets:\n",
    "    all_keyps = dets['all_keyps'][1]\n",
    "else:\n",
    "    all_keyps = None\n",
    "if 'all_tracks' in dets:\n",
    "    all_tracks = dets['all_tracks'][1]\n",
    "else:\n",
    "    all_tracks = None\n",
    "    \n",
    "all_tracks_np = np.array(all_tracks)\n",
    "n_tracks = np.amax(all_tracks_np)[0] + 1\n",
    "n_frames = len(all_tracks)\n",
    "print('There are {} individual tracks  in {} total number of frames'.format(n_tracks, n_frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract all the tracklets in this format: [[[st0], [list of boxes]], [[st1], [list of boxes]], …..]\n",
    "### Extract the highest confidence bounding box from each tracklet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remaining list of IDs after pruning based on track length, number of keypoints in detection and lowest confidence of 0.94 is:\n",
      "[0, 5, 12, 17, 19, 21, 49, 51, 69, 74, 78, 80, 82, 83, 85, 87, 90, 92, 93, 98, 99, 108, 114, 120, 123, 132, 139, 140, 143, 148, 150, 151, 174, 175, 176, 180, 182, 183, 187, 188, 189, 191, 197, 199, 209, 211, 216, 220, 223, 224, 225, 226, 229, 232, 233, 235, 236, 243, 249, 251, 252, 254, 255, 257, 262, 267, 270, 272, 276, 278, 279, 282, 284, 295, 306, 311, 318, 320, 322, 324, 325, 326, 328, 332, 333, 335, 337, 338, 339, 341, 342, 343, 344, 345, 346, 347, 348, 349, 352, 354, 355, 356, 361, 379, 384, 385, 418, 419, 423, 426, 459, 477, 481, 487, 489, 491, 493, 500, 502, 505, 506, 511, 561, 574, 604, 621, 622, 625, 629, 630, 631, 634, 635, 639, 640, 642, 644, 658, 660, 668, 670, 671, 672, 674, 684, 701, 708, 711, 712, 715, 722, 737, 742, 746, 747, 758, 759, 761, 763, 766, 767, 770, 771, 778, 779, 783, 784, 787, 788, 789, 794, 796, 797, 802, 803, 805, 810, 811, 846, 847, 848, 849, 852, 853, 855, 858, 860, 862, 868, 877, 878, 879, 880, 885, 886, 887, 888, 891, 896, 906, 910, 913]\n"
     ]
    }
   ],
   "source": [
    "tracklet_list =[]\n",
    "id_list =[]\n",
    "th_box_conf = 0.94\n",
    "# it is not the optimized way to create the tracklet_list\n",
    "# TODO: change the search mw=ethod to a more optimized scheme\n",
    "for i in range(n_tracks):\n",
    "    end_fr, st_fr, target_box = _get_target_tracklet(i , all_boxes, all_tracks, st_fr = None)\n",
    "    hconf_box_ind = np.argmax(np.array(target_box)[:, 4])\n",
    "    hconf_box = np.max(np.array(target_box)[:, 4])\n",
    "    key_pts = all_keyps[st_fr+hconf_box_ind][all_tracks[st_fr+hconf_box_ind].index(i)]\n",
    "    key_pts_conf = key_pts[2,:]\n",
    "    key_pts_num = sum(key_pts_conf > 2.3)\n",
    "    # pruning based on the detection confidence, number of key points and tracklet length\n",
    "    length = end_fr - st_fr\n",
    "    if hconf_box > th_box_conf and length > 8 and key_pts_num > 6:\n",
    "        tracklet_list.append([st_fr, i, hconf_box_ind, target_box])\n",
    "        id_list.append(i)\n",
    "#second level of pruning based on overlaps with other bounding boxes\n",
    "print('remaining list of IDs after pruning based on track length, number of keypoints in detection '\n",
    "      'and lowest confidence of {} is:\\n{}'.format(th_box_conf, id_list) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data frame from the list: [[fr#, track_id, [bbox]]], [fr#, track_id, [bbox]], …]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202 tracklets are remained after pruning\n"
     ]
    }
   ],
   "source": [
    "data_list =[]\n",
    "for info in tracklet_list:\n",
    "    #print(info)\n",
    "    st = info[0]\n",
    "    max_ind = info[2]\n",
    "    track_id =info[1]\n",
    "    fr_no = st + max_ind\n",
    "    data_list.append([fr_no, track_id, info[3][max_ind][0:4]])\n",
    "print('{} tracklets are remained after pruning'.format(len(data_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the patient related track ids from the ground truth folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traklet IDs corrresponding to the target are:\n",
      "[0, 92, 99, 108, 123, 132, 279, 282, 318, 320, 324, 344, 355, 361, 658, 660, 668, 670, 671, 672, 674, 684, 701, 722, 737, 742, 747, 758, 761, 767, 771, 779, 784, 788, 796, 797, 803, 805, 811, 846, 880, 886, 888, 906]\n",
      "Number of generated tracklets for the target is:44\n"
     ]
    }
   ],
   "source": [
    "# read the true labels from files\n",
    "base_path = '../gt_tracking_labels/TuftsVideos'\n",
    "true_id_path = osp.join(base_path, subj_id, 'Visit_%02d' % visit_no)\n",
    "true_id = {}\n",
    "patient_list = [ i for i in os.listdir(osp.join(true_id_path, 'patient')) if i.endswith('.jpg')]\n",
    "for name in patient_list:\n",
    "    track_id = int(name.split('_')[0])\n",
    "    true_id[track_id] = 0\n",
    "true_id_list = [track_id for (track_id, label) in sorted(true_id.items(), reverse=False) \n",
    "                if find_element_in_list(track_id, id_list) is not None ] \n",
    "print('Traklet IDs corrresponding to the target are:\\n{}'\n",
    "      '\\nNumber of generated tracklets for the target is:{}'.format(true_id_list, len(true_id_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the final detection tracking results for the selected target\n",
    "if in a frame there are more than one detection associated to the same identity the one which has the smaller bbox area will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tracks_pruned = []\n",
    "all_boxes_pruned = []\n",
    "all_keyps_pruned = []\n",
    "for i in range(n_frames):\n",
    "    track_ids = all_tracks[i]\n",
    "    idx = 0\n",
    "    tracks_pruned = []\n",
    "    boxes_pruned = []\n",
    "    keyps_pruned = []\n",
    "    for j in track_ids:\n",
    "        if find_element_in_list(j, true_id_list) is not None:\n",
    "            tracks_pruned.append(j)\n",
    "            boxes_pruned.append(np.expand_dims(all_boxes[i][idx],axis=0))\n",
    "            keyps_pruned.append(all_keyps[i][idx])\n",
    "        idx += 1\n",
    "    if len(tracks_pruned) > 1:\n",
    "        area_pruned = [boxes_area(boxes_pruned[i]) for i in range(len(boxes_pruned))]\n",
    "        keep_id = area_pruned.index(max(area_pruned))\n",
    "        tracks_pruned = tracks_pruned[keep_id]\n",
    "        boxes_pruned = boxes_pruned[keep_id]\n",
    "        keyps_pruned = keyps_pruned[keep_id]\n",
    "    all_tracks_pruned.append(tracks_pruned)\n",
    "    all_boxes_pruned.append(boxes_pruned)\n",
    "    all_keyps_pruned.append(keyps_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing back the final results of detections for the target \n",
    "all results for detected bounding boxes and estimated keypoints will be written in a .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = osp.join('../outputs/TuftsVideos',subj_id, 'Visit_%02d' % visit_no)\n",
    "dest_name = file_name.split('_')[0] + '_' + file_name.split('_')[1]+ '_target_tracking.pkl'\n",
    "dest_file = osp.join(out_path, dest_name)\n",
    "if osp.exists(out_path):\n",
    "    shutil.rmtree(out_path)\n",
    "os.makedirs(out_path)\n",
    "robust_pickle_dump(\n",
    "    dict(target_boxes=all_boxes_pruned,\n",
    "         target_keyps=all_keyps_pruned),\n",
    "    dest_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
