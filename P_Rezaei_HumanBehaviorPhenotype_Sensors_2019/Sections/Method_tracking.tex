%============ method_tracking
In order to track the pose of a target human actor (the subject in our dataset) in recorded videos to be used further in the action classification system, we propose an incremental procedure using the estimated bounding boxes and keypoints provided by the pose estimation network in \secref{PoseEst}. As illustrated in \figref{Tracking}, in the first stage we use a lightweight data association approach to link the detected bounding boxes to the tracklets. Tracklets are a series of bounding boxes in consecutive frames associated with the same identity (person). This stage is called short-term tracking and is described in more details in \secref{Hungarian}. In the following stages, we fuse the generated tracklets for the same identity from previous stage using their learned appearance features to provide long-term tracking for the target actor. The implementation details are described in \secref{Fusion}.
\\
\subsubsection{Short term tracking based on time information}
\Tracking
\label{sec:Hungarian}
Given the detected bounding boxes for each person in the video, we link the bounding boxes that belong to the same identity in time to create pose tracklets. Assuming that there is no abrupt movement in the video, tracklets are generated by solving a data association problem with similarity measurement defined as the intersection over union between the currently detected bounding boxes and the bounding boxes from the previous frame. Like \cite{girdhar2018detect, pirsiavash2011globally}, we formulate the task as a bipartite matching problem, and solve it using the
Hungarian algorithm \cite{kuhn2005hungarian}. We initialize tracklets on the first frame and propagate the labels forward one frame at a time using the matches. Any box that does not get matched to an existing tracklet instantiates a new tracklet. This method is time efficient and can be adapted to any video length or any number of people. However, tracking can fail due to challenges such as abrupt camera motion, occlusions and change of scene, which can result in multiple tracklets for the same identity. For instance, as illustrated in \figref{Tracking}, the short term tracking at this early stage generates 3 distinct tracklets for the target patient in 700 consecutive frames.
\\
\subsubsection{Long term tracking using appearance based tracklet fusion}
\label{sec:Fusion}
Given the large number of tracklets generated from the previous stage, we fuse tracklets that belong to the same identity to generate a single long-term track for the target human actor. As illustrated in \figref{Tracking}, in order to merge the generated tracklets belonging to the same identity throughout the video, we first apply sparse sampling by pruning the tracklets based on their length and the number of estimated keypoints, and then selecting the highest confidence bounding box from each tracklet. Finally, we merge the tracklets into a single track based on their similarity to the reference tracklet. The affinity metric between the tracklet $T_i$, and the reference tracklet $T_{ref}$, is calculated as:

\begin{align}\label{eqn:affinity}
 P_a(T_i, T_{ref}) = ||f^{t'}_i - f^{t}_{ref}||_2   
\end{align}
%
where $f^{t'}_i$ is feature vector of the sampled detection in tracklet $T_i$ at time $t'$, and $f^{t}_{ref}$ is feature vector of the sampled detection in reference tracklet $T_{ref}$ at time $t$. Affinity metric, $P_a(.)$ is the Euclidean distance between the above feature vectors. In order to extract deep appearance features, we feed every sampled detection of each tracklet to the base network of a Mask R-CNN (i.e. ResNet-101), which has been trained on the PoseTrack dataset for pose estimation \cite{he2017mask, he2016deep}. The extracted feature map is then aligned spatially to a fixed resolution via ROI-align operation. It is worth mentioning that we do not pay an extra cost for learning the features for merging the associated tracklets of the target human actor into one track.


