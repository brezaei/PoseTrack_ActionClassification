
In order to extract human pose information in each video frame along with their associated bounding boxes as the first step in our human behavior phenotyping system, we used a 2D version of the state-of-the-art human pose estimation method proposed in \cite{girdhar2018detect}. This method performs efficient frame level multi-person pose estimation in videos benefiting from the Mask R-CNN network \cite{he2017mask}. The architecture of this pose estimation network is illustrated in \figref{Pose}. The network uses ResNet-101 \cite{he2016deep} as the base convolutional network for extracting image features. Extracted features are then fed to a trained region proposal network (RPN) to highlight regions that contain object candidates \cite{ren2017faster}. Candidate regions of the output feature map are all aligned to a fixed resolution via a spatial region of interest (ROI)-align operation. This operation divides feature maps that may have different sizes depending on the size of detected bounding boxes to a fixed number of sub-windows. The value for each sub-window is calculated by finding a bi-linear interpolation of four regularly sampled locations inside the sub-window. Aligned features are then fed into two heads, a classification head responsible for person detection and bounding box regression, and a keypoint head for estimating the human body joints defined as human pose in each detected bounding box. The outputs of this pose estimation network are seventeen keypoints associated with various body joints and a bounding box surrounding each person. This model was initialized from ImageNet \cite{deng2009imagenet} and pre-trained on the COCO keypoint detection task \cite{lin2014microsoft}. The authors then fine-tuned the Mask R-CNN network on the PoseTrack dataset \cite{andriluka2018posetrack}.

