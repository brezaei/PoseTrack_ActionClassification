%============== Experiments

\subsection{Evaluation Dataset} \label{sec:dataset}
The dataset used in this study consists of video recordings of patients with Parkinson's disease who participated in a clinical study to assess changes in their motor symptoms before (OFF state) and after (ON state) medication intake. The study had approval from the Tufts Medical Center and Tufts University Health Sciences Institutional Review Board and all experimental procedures were conducted at Tufts Medical Center \cite{erb2018bluesky}.

The study protocol included two visits to the clinic; subjects were randomly assigned to be in the ON (after medication intake) or OFF (before medication intake) state for the first visit, and underwent the second visit in the other state. During each study visit, patients performed a battery of motor tasks including activities of daily living (e.g. dressing, writing, drinking from a cup of water, opening a door, folding clothes) and a standard battery of clinical motor assessments from the Movement Disorder Society's Unified Parkinson's Disease Rating Scale (MDS-UPDRS) \cite{goetz2008movement} administered by a trained clinician with experience in movement disorders. Each visit lasted approximately 1 hour and most of the experimental activities were video recorded at 30 frames per second by two Microsoft Kinect\textsuperscript{TM} cameras (640 x 480-pixel resolution), one mounted on a mobile tripod in the testing room and another on a wall mount in the adjacent hallway. In total, the dataset consists of 70 video recordings (35 subjects $\times$ 2 visits per subject). The video camera was positioned to capture a frontal view of the subject at most times. Besides the subject, there are several other people (e.g. physicians, nurses, study staff) who appeared in these video recordings.

Behaviors of interest were identified within each video using structured definitions and, their start and end times annotated using human raters as described elsewhere \cite{brooks2019}. Briefly, each video recording was reviewed and key behaviors annotated by two trained raters. To maximize inter-rater agreement, each behavior had been explicitly defined to establish specific, anatomically based visual cues for annotating its start and end times. The completed annotations were reviewed for agreement by an experienced arbitrator, who identified and resolved inter-rater disagreements (e.g. different start times for a behavior). The annotated behaviors were categorized into three classes: postures (e.g. walking, sitting, standing), transitions (e.g. sit-to-stand, turning), and cued behaviors (i.e. activities of daily living and MDS-UPDRS tasks). In this manuscript, we focus on the recognition of postures (sitting, standing and walking) and transitions (sitting-to-standing and standing-to-sitting). The major challenges in recognizing the target human (subject) activities in this dataset were camera motion (when not on tripod), change of scene as the experimental activities took place in different environments (e.g. physician office, clinic hallway, etc.) and long periods occlusion (around a few minutes) due to interactions between the patient and the study staff.

\subsection{Tracking Target Human and Pose} \label{sec:trackResult}
Given that video recordings involved the presence of multiple people, we first detected all human actors along with their associated keypoints in each video frame using the multi-person pose estimation method described in \secref{PoseEst}. This pose estimation network was pre-trained on the COCO dataset and fine-tuned on the PoseTrack dataset by the authors \cite{lin2014microsoft,andriluka2018posetrack, girdhar2018detect}. As illustrated in \figref{Tracking}, the output of this stage is a list of the bounding boxes for human actors detected in each video frame and the estimated locations of keypoints for each person along with a confidence estimate for each keypoint.

In order to recognize activities of the target human actor (i.e. the subject in our dataset), we first locate and track the patient in each frame. We accomplish this by using the proposed hierarchical tracking method described in \secref{track}. Given all detected bounding boxes across all frames from the pose estimation stage, we first generate tracklets for each identity appearing in the video via short-term tracking explained in \secref{Hungarian}. Each tracklet is a list of detected bounding boxes in consecutive frames that belong to the same identity. In order to find the final patient track for the entire video, we use the long-term tracking method described in \secref{Fusion} to remove non-target tracklets (e.g. study staff, physician, nurse) and fuse the tracklets that belong to the patient using the appearance features. There is no supervision in tracking of the patient during the video except providing a reference tracklet, which is associated to the patient (target human actor) in the long-term tracking step.

To evaluate the performance of our target human actor tracking method, we first manually annotated all tracklets generated by short-term tracking and then calculated accuracy of the long-term tracking method with respect to the manually generated ground-truth. Accuracy is calculated by treating the long-term tracker as a binary classifier as it excludes non-patient tracklets and fuses tracklets belonging to the target human actor to find a single final patient track for the entire video recording. Considering patient tracklets as the positive class and non-patient tracklets as the negative class, our tracker achieved average classification accuracy of 88\% across 70 videos on this dataset.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Action Classification} \label{sec:actionresult}
\dataDist
\accuracy
In the last stage of our multi-stage target-specific action classification system, we train a CNN to recognize the following five actions of interest: sitting, standing, walking, sitting-to-standing, and standing-to-sitting. After applying the target actor pose tracking system illustrated in \figref{Tracking}, we segmented the resulting long-term video into action clips based on ground-truth annotations provided by our human raters. Although the action clips have variable lengths (ranging from a few frames to more than 10 minutes), each video clip includes only one of the five actions of interest. As a result, we ended up with a highly imbalanced dataset. In order to create a more balanced dataset for training and evaluating the action classification network, we first excluded action clips less than 0.2 sec and divided the ones longer than 4 seconds into 4-second clips. Assuming that 4 seconds is long enough for an action to have taken place by a person and below 0.2 seconds (lower than 6 frames) is too short to be used for recognizing an action \cite{barrouillet2004time}.  This resulted in a total of 44580 action clips extracted from video recordings of 35 subjects. We used 29 subjects (39086 action clips) for training/validation set and the remaining 6 subjects (5494 action clips) were held out for testing. As shown in \figref{dataDist}, the resulting dataset is highly imbalanced with a significant skew towards the sitting class, which can result in over-fitting issues. To address this imbalance, we randomly under-sampled the walking, sitting, and standing classes to 4000 video clips each.

To prepare input data for the action classification network, we transformed each action clip into a pose evolution representation as described in \secref{potion}. To create the pose evolution maps, we scaled the original size of each video frame ($1080 \times 1920$) by a factor of $0.125$ and chose $3$ channels to represent the input based on training time and average validation accuracy in diagnostic experiments. The training dataset was also augmented by adding Gaussian noise. In addition, we tried data augmentations techniques like random translation and flipping during our diagnostic experiments, but they were not beneficial. The augmented volumetric pose evolution representations were fed to the classification network explained in \secref{network} for learning the network parameters and testing the classification performance.

We used $90\%$ of the train/validation dataset for training the action classification network with architecture illustrated in \figref{ActionNet} and the rest for validation. Network training started with random weight initialization, used the Adam optimizer with a base learning rate of $0.01$. We used a batch size of $70$ and a dropout probability of $0.3$. The trained action classification network achieved average classification accuracy of $84\%$ in test dataset. More details of the classification performance including per class accuracy in the test and validations phase can be found in \tblref{accuracy}.
\Cmat
\chStat

Furthermore, we experimented with several variants of the network architecture proposed in \secref{action} by increasing the number of the convolution blocks to three and changing the number of filters in each block between 64, 128, 256, and 512. Based on the performance on the validation set and training loss, \figref{ActionNet} provided the best performance while avoiding over-fitting to the training data. In addition, we investigated the impact of using a different number of channels for representing the temporal pose evolution on the performance of action classification. \figref{chStat} illustrates the accuracy of the classification network with different representations as input. We chose 3 channels for our representation as a trade-off between training time and network performance.