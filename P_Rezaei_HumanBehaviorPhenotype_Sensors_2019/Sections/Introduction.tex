%====================== Introduction
Clinical assessment of human motor behavior plays an important role in the diagnosis and management of medical conditions like Parkinson's Disease (PD) \cite{post2005unified}. However, such assessments can only be performed intermittently by trained individuals, which limits the quantity and quality of information that can be collected to understand the impact of disease in the real-world setting. To address these limitations, significant efforts have been made to develop wearable sensing technologies that can be used for continuously monitoring various types of motor symptoms and behaviors \cite{thorp2018monitoring, lara2013survey}. While data collected using wearable sensors are well suited for detecting and measuring basic movements (e.g. arm or leg movements, tremor) and actions (e.g. sitting, standing, walking), they are ill-suited when it comes to complex activities (e.g. cooking, grooming) and behaviors (e.g. personal habits, routines) - particularly if they involve the interpretation of environmental interactions (e.g. with other humans, animals, or objects).
\taxonomy
Recently, artificial intelligence (AI) assisted human behavior phenotyping using computer vision has received newfound attention among researchers in machine learning and pattern recognition communities for applications spanning from automatic recognition of daily life activities in smart homes to monitoring the health and safety of elderly and patients with mobility disorders in their homes/hospitals \cite{vrigkas2015review,chaaraoui2012review,chen2018robust,ostadabbas2016vision}. Vision-based assessment of human behavior enables us to automate the detection and measurement of the full range of human behaviors. As illustrated in \figref{taxonomy}, the taxonomy of human behaviors can be viewed as a four-level hierarchical framework with basic movements at the bottom (e.g. movement of body segments) and complex behaviours (e.g. personal habits and routines) at the top. Automatic recognition at any level requires that actions and/or behaviors at the level below it are also recognized. For example, in order to recognize walking, we first need to assess if the pose is upright, the arms are swinging and legs are moving. At the first level (motion), recognition deals with tasks such as movement detection or background extraction/segmentation in video recordings of the target human actor \cite{rezaei2017background, 7952497, 8456638}. These techniques try to locate the moving objects in a scene by extracting a silhouette of the object in a single frame or over a few consecutive frames. However, segmentation algorithms without any further processing provide only very basic pose estimation of the object with little to no temporal information. At the second level (action), human motions along with environmental interactions are classified in order to recognize what a person is doing over a period of seconds or minutes \cite{herath2017going}. At the third level (activity), the recognition task is focused on identifying activities as a combination of sequence of actions and environmental interactions over a period of minutes to hours. Finally, at the fourth level (behavior), sequence of human activities and environmental interactions along with information about their temporal dependencies are used to recognize complex human behaviors.


Recognizing activities that lead to phenotyping of human behavior in multi-person video requires the tracking and classification of a sequence of actions performed by a target human actor (e.g. patient). Therefore, accurate temporal tracking of the target human actor is an essential requirement for this application, along with robust feature extraction that can be used for classifying human behaviors at different levels of complexity. In this paper, we present a hierarchical target-specific action classification method as illustrated in \figref{Method}. Detection of different actions performed by the target human actor (i.e. patient) is done using pose evolution feature representation. We define pose evolution as a low-dimensional embedding of a sequence of posture movements related to an action. In order to find a relevant pose evolution feature representation, we present an incremental target pose tracking algorithm that receives multi-person pose estimation results from an earlier stage. Our main contributions in this paper are: (1) development of a robust hierarchical multiple-target pose tracking method to facilitate action recognition in videos recorded in uncontrolled environments with multiple human actors; (2) introducing pose evolution, an explicit body movement representation, as complementary information to the appearance and motion cues for robust action recognition; and (3) a novel target-specific action recognition architecture in untrimmed videos.

\subsection{Related works}
\MethodOverview
The need for person-centric action recognition in which each person is labeled to a specific action rather than associating an action to a video or clip is quite common in the videos recorded in real-world applications such as vision based human behavior phenotyping. One of the challenges in person-centric action recognition is robust tracking of the target human actor in long-term videos. Tracking is challenging because there are many sources of uncertainty, such as clutter, serious occlusions, target interactions, and camera motion. However, most of the research studies on human activity classification have typically dealt with videos with a single human actor or video clips with ground-truth tracking provided \cite{dawar2018data}, with the exception of few that performed human-centeric action recognition \cite{girdhar2018video,chen2018robust}. Girdhar, et al. \cite{girdhar2018video} re-purposed an action transformer network to exclude non-target human actors in the scene and aggregated spatio-temporal features around the target human actor. Chen, et al. \cite{chen2018robust} presented human activity classification using skeleton motions in videos with interference from non-target objects aimed at supporting applications in monitoring frail and elderly individuals. However, neither work provided details on how they addressed non-target filtering in their human action classification pipelines. 

Beside the importance of removing non-actor objects in providing a well-performing real-world human action recognition system, creating robust and discriminating feature representations for each video action clip plays an important role in detecting different human activities. Most of the state-of-the-art action recognition architectures process appearance and motion cues in two independent streams of information, which are fused right before the classification phase or a few stages before the classification stage in a merge and divide scheme \cite{li2018detecting, simonyan2014two}. Others have used 3D spatio-temporal convolutions to directly extract relevant spatial and temporal features \cite{zhou2018mict, tran2018closer, tran2015learning}. However, human pose cues, which can provide low-dimensional interpretations for different activities, have been overlooked in these studies. 
Most recently, Choutas, et al. \cite{liu2018recognizing} and Mengyuan, et al. \cite{choutas2018potion} have used temporal changes of pose information with two different representations for boosting action recognition performance. In \cite{choutas2018potion}, authors claim that if there are multiple people in the scene, pose motion representation does not need the time associations of the joints to work but they did not address how their proposed method can handle multiple human actors in a video.

In general, convolutional neural network (CNN) based action recognition task can be divided into three different categories based on their underlying architecture: (1) spatio-temporal convolutions (3-dimensional convolutions), (2) recurrent neural networks, and (3) two stream convolutional networks.
The benefit of multi-stream networks is that different modalities can be aggregated in the network to improve performance of the final action classification task. In this paper, we addressed the problem of person-centric action recognition by long-term tracking of the target human actor. In addition, our method provides a novel pose evolution representation of the target human actor rather than the common spatio-temporal features extracted from raw video frames to the classification network. It is worth mentioning that our pose-based action recognition stream can be used as an augmentation to the current multi-stream action classification networks.

The rest of the paper is organized as follows. In \secref{PoseTrack}, we describe the proposed method for tracking target human actor in untrimmed videos in order to extract appropriate pose evolution features from located action in video. In \secref{action}, we describe the subsequent stages for action classification in \figref{Method}, consisting of the pose evolution feature representation and classification network. We present our experimental setup and performance evaluation results of the proposed method in \secref{experiment}. Finally, we discuss the results in \secref{discussion} and conclude our paper in \secref{conclusion}.